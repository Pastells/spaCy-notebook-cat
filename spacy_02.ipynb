{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anàlisi de dades a gran escala amb spaCy\n",
    "\n",
    "https://course.spacy.io/chapter2\n",
    "\n",
    "En aquest capítol, utilitzaràs les teves noves habilitats per extreure informació específica de grans volums de text. Aprendràs com treure el màxim profit de les estructures de dades de spaCy, i com combinar eficaçment enfocaments estadístics i basats en regles per a l'anàlisi de text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Estructures de dades (1): Vocab, Lexemes i StringStore\n",
    "\n",
    "Ara que ja tens experiència real utilitzant els objectes de spaCy, és hora que aprenguis més sobre què passa realment sota el capó de spaCy.\n",
    "\n",
    "En aquesta lliçó, farem un cop d'ull al vocabulari compartit i com spaCy gestiona les cadenes de text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulari compartit i emmagatzematge de cadenes\n",
    "\n",
    "SpaCy emmagatzema totes les dades compartides en un vocabulari, el Vocab.\n",
    "\n",
    "Això inclou paraules, però també els esquemes d'etiquetes per a etiquetes i entitats.\n",
    "\n",
    "Per estalviar memòria, totes les cadenes es codifiquen com a IDs de hash. Si una paraula apareix més d'una vegada, no cal guardar-la cada vegada.\n",
    "\n",
    "En canvi, spaCy utilitza una funció de hash per generar un ID i emmagatzemar la cadena només una vegada a l'StringStore.\n",
    "\n",
    "L'string store està disponible com a nlp.vocab.strings.\n",
    "\n",
    "És una taula de cerca que funciona en ambdues direccions. Pots cercar una cadena per obtenir el seu hash, o un hash per obtenir la seva cadena.\n",
    "\n",
    "Internament, spaCy només es comunica amb IDs de hash."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Vocab: emmagatzema dades compartides entre múltiples documents\n",
    "- Per estalviar memòria, spaCy codifica totes les cadenes com a hash IDs\n",
    "- Les cadenes només s'emmagatzemen una vegada a l'StringStore mitjançant nlp.vocab.strings\n",
    "- String store: taula de cerca en ambdues direccions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "nlp.vocab.strings.add(\"coffee\")\n",
    "coffee_hash = nlp.vocab.strings[\"coffee\"]\n",
    "coffee_string = nlp.vocab.strings[coffee_hash]\n",
    "\n",
    "print(coffee_hash)\n",
    "print(coffee_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Els IDs de hash no es poden revertir. Si una paraula no és al vocabulari, no hi ha manera d'obtenir la seva cadena. Per això sempre hem de passar el vocabulari compartit.\n",
    "\n",
    "- Els hashes no es poden revertir – per això hem de proporcionar el vocabulari compartit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Això genera un error si no hem vist la cadena abans\n",
    "string = nlp.vocab.strings[3197928453018144401]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"I love coffee\")\n",
    "print(\"hash value:\", nlp.vocab.strings[\"coffee\"])\n",
    "print(\"string value:\", nlp.vocab.strings[3197928453018144401])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El doc també exposa el vocabulari i les cadenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"I love coffee\")\n",
    "\n",
    "print(\"hash value:\", doc.vocab.strings[\"coffee\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lexemes: entrades al vocabulari\n",
    "\n",
    "Els lexemes són entrades al vocabulari independents del context.\n",
    "\n",
    "Obtens un lexeme cercant una cadena o un ID de hash al vocabulari.\n",
    "\n",
    "Els lexemes exposen atributs, igual que els tokens.\n",
    "\n",
    "Contenen informació independent del context sobre una paraula.\n",
    "\n",
    "Text de la paraula: lexeme.text i lexeme.orth (el hash).\n",
    "    \n",
    "Atributs lèxics com lexeme.is_alpha.\n",
    "\n",
    "No conté etiquetes de part del discurs, dependències o etiquetes d'entitat dependents del context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "img src=\"img/data-struct.png\" - s'han eliminat les etiquetes per raons de compatibilitat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un exemple a dalt:\n",
    "\n",
    "El Doc conté paraules en context – en aquest cas, els tokens \"I\", \"love\" i \"coffee\" amb les seves etiquetes de part del discurs i dependències.\n",
    "\n",
    "Cada token fa referència a un lexeme, que coneix l'ID de hash de la paraula. Per obtenir la representació en cadena de la paraula, spaCy cerca el hash a l'string store."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cadenes a hashes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1\n",
    "\n",
    "Cerca la cadena \"cat\" a nlp.vocab.strings per obtenir el hash.\n",
    "\n",
    "Cerca el hash per recuperar la cadena."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"I have a cat\")\n",
    "\n",
    "# Cerca el hash per a la paraula \"cat\"\n",
    "cat_hash = nlp.vocab.strings[\"cat\"]\n",
    "print(cat_hash)\n",
    "\n",
    "# Cerca cat_hash per obtenir la cadena\n",
    "cat_string = nlp.vocab.strings[cat_hash]\n",
    "print(cat_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2\n",
    "\n",
    "Cerca l'etiqueta de cadena \"PERSON\" a nlp.vocab.strings per obtenir el hash.\n",
    "\n",
    "Cerca el hash per recuperar la cadena."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"David Bowie is a PERSON\")\n",
    "\n",
    "# Cerca el hash per a la cadena \"PERSON\"\n",
    "person_hash = nlp.vocab.strings[\"PERSON\"]\n",
    "print(person_hash)\n",
    "\n",
    "# Cerca person_hash per obtenir la cadena\n",
    "person_string = nlp.vocab.strings[person_hash]\n",
    "print(person_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Vocab, hashes i lexemes\n",
    "\n",
    "Per què aquest codi genera un error?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.de import German\n",
    "from spacy.lang.en import English\n",
    "\n",
    "# Crea objectes nlp en anglès i alemany\n",
    "nlp = English()\n",
    "nlp_de = German()\n",
    "\n",
    "# Obté l'ID per a la cadena 'Bowie'\n",
    "bowie_id = nlp.vocab.strings[\"Bowie\"]\n",
    "print(bowie_id)\n",
    "\n",
    "# Cerca l'ID per a 'Bowie' al vocabulari\n",
    "print(nlp_de.vocab.strings[bowie_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La cadena 'Bowie' no és al vocabulari alemany, així que el hash no es pot resoldre a l'string store.\n",
    "\n",
    "Els hashes no es poden revertir. Per prevenir aquest problema, afegeix la paraula al nou vocabulari processant un text o cercant la cadena, o utilitza el mateix vocabulari per resoldre el hash a una cadena."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Estructures de dades (2)\n",
    "\n",
    "Ara que saps tot sobre el vocabulari i l'string store, podem fer un cop d'ull a l'estructura de dades més important: el Doc, i les seves vistes Token i Span."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L'objecte Doc\n",
    "\n",
    "El Doc és una de les estructures de dades centrals a spaCy. Es crea automàticament quan processes un text amb l'objecte nlp. Però també pots instanciar la classe manualment.\n",
    "\n",
    "Després de crear l'objecte nlp, podem importar la classe Doc des de spacy.tokens.\n",
    "\n",
    "Aquí creem un doc a partir de tres paraules. Els espais són una llista de valors booleans que indiquen si la paraula va seguida d'un espai. Cada token inclou aquesta informació – fins i tot l'últim!\n",
    "\n",
    "La classe Doc pren tres arguments: el vocabulari compartit, les paraules i els espais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crea un objecte nlp\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Importa la classe Doc\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "# Les paraules i espais per crear el doc\n",
    "words = [\"Hello\", \"world\", \"!\"]\n",
    "spaces = [True, False, False]\n",
    "\n",
    "# Crea un doc manualment\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L'objecte Span\n",
    "\n",
    "Un Span és una porció d'un document que consisteix en un o més tokens. L'Span pren almenys tres arguments: el doc al qual fa referència, i els índexs d'inici i final del span. Recorda que l'índex final és exclusiu!\n",
    "\n",
    "Per crear un Span manualment, també podem importar la classe des de spacy.tokens. Després podem instanciar-lo amb el doc i els índexs d'inici i final del span, i una etiqueta opcional.\n",
    "\n",
    "Els doc.ents són escribibles, així que podem afegir entitats manualment sobreescrivint-los amb una llista de spans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa les classes Doc i Span\n",
    "from spacy.tokens import Doc, Span\n",
    "\n",
    "# Les paraules i espais per crear el doc\n",
    "words = [\"Hello\", \"world\", \"!\"]\n",
    "spaces = [True, False, False]\n",
    "\n",
    "# Crea un doc manualment\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "\n",
    "# Crea un span manualment\n",
    "span = Span(doc, 0, 2)\n",
    "\n",
    "# Crea un span amb una etiqueta\n",
    "span_with_label = Span(doc, 0, 2, label=\"GREETING\")\n",
    "\n",
    "# Afegeix el span a les entitats del doc\n",
    "doc.ents = [span_with_label]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alguns consells i trucs abans de començar:\n",
    "\n",
    "El Doc i l'Span són molt potents i estan optimitzats per al rendiment. Et donen accés a totes les referències i relacions de les paraules i frases.\n",
    "\n",
    "Si la teva aplicació necessita produir cadenes, assegura't de convertir el doc el més tard possible. Si ho fas massa aviat, perdràs totes les relacions entre els tokens.\n",
    "\n",
    "Per mantenir la consistència, intenta utilitzar els atributs de token integrats sempre que sigui possible. Per exemple, token.i per a l'índex del token.\n",
    "\n",
    "A més, no oblidis passar sempre el vocabulari compartit!\n",
    "\n",
    "<b> Bones pràctiques: </b>\n",
    "\n",
    "* Doc i Span són molt potents i contenen referències i relacions de paraules i frases\n",
    "\n",
    "* Converteix els resultats a cadenes el més tard possible\n",
    "    \n",
    "* Utilitza atributs de token si estan disponibles – per exemple, token.i per a l'índex del token\n",
    "\n",
    "* No oblidis passar el vocabulari compartit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Creant un Doc\n",
    "\n",
    "Creem alguns objectes Doc des de zero!\n",
    "\n",
    "### Part 1\n",
    "\n",
    "Importa el Doc des de spacy.tokens.\n",
    "\n",
    "Crea un Doc a partir de les paraules i espais. No oblidis passar el vocabulari!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Importa la classe Doc\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "# Text desitjat: \"spaCy is cool!\"\n",
    "words = [\"spaCy\", \"is\", \"cool\", \"!\"]\n",
    "spaces = [True, True, False, False]\n",
    "\n",
    "# Crea un Doc a partir de les paraules i espais\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "print(doc.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2\n",
    "\n",
    "Importa el Doc des de spacy.tokens.\n",
    "\n",
    "Crea un Doc a partir de les paraules i espais. No oblidis passar el vocabulari!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Importa la classe Doc\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "# Text desitjat: \"Go, get started!\"\n",
    "words = [\"Go\", \",\", \"get\", \"started\", \"!\"]\n",
    "spaces = [False, True, True, False, False]\n",
    "\n",
    "# Crea un Doc a partir de les paraules i espais\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "print(doc.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Docstring del DOC: </b>\n",
    "    \n",
    "Una seqüència d'objectes Token. Accedeix a frases i entitats anomenades, exporta\n",
    "anotacions a arrays de numpy, serialitza sense pèrdues a cadenes binàries\n",
    "comprimides. L'objecte `Doc` conté un array d'estructures `TokenC`. Els\n",
    "objectes `Token` i `Span` a nivell de Python són vistes d'aquest array, és a dir,\n",
    "no posseeixen les dades ells mateixos.\n",
    "\n",
    "EXEMPLE: Construcció 1\n",
    "    >>> doc = nlp(u'Some text')\n",
    "\n",
    "    Construcció 2\n",
    "    >>> from spacy.tokens import Doc\n",
    "    >>> doc = Doc(nlp.vocab, words=[u'hello', u'world', u'!'],\n",
    "                  spaces=[True, False, False])\n",
    "Docstring d'inicialització:\n",
    "Crea un objecte Doc.\n",
    "\n",
    "vocab (Vocab): Un objecte de vocabulari, que ha de coincidir amb qualsevol model que\n",
    "    vulguis utilitzar (per exemple, tokenitzador, analitzador sintàctic, reconeixedor d'entitats).\n",
    "    \n",
    "words (list o None): Una llista de cadenes unicode per afegir al document\n",
    "    com a paraules. Si és `None`, per defecte és una llista buida.\n",
    "    \n",
    "spaces (list o None): Una llista de valors booleans, de la mateixa longitud que\n",
    "    words. True significa que la paraula va seguida d'un espai, False significa\n",
    "    que no. Si és `None`, per defecte és `[True]*len(words)`\n",
    "    \n",
    "user_data (dict o None): Dades opcionals del programa, atributs no estructurats\n",
    "    (sense semàntica) adjunts al Doc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3\n",
    "\n",
    "Importa el Doc des de spacy.tokens.\n",
    "\n",
    "Crea un Doc a partir de les paraules i espais. No oblidis passar el vocabulari!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Importa la classe Doc\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "# Text desitjat: \"Oh, really?!\"\n",
    "words = [\"Oh\", \",\", \"really\", \"?\", \"!\"]\n",
    "spaces = [False, True, False, False, False]\n",
    "\n",
    "# Crea un Doc a partir de les paraules i espais\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "print(doc.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Docs, spans i entitats des de zero\n",
    "\n",
    "En aquest exercici, crearàs els objectes Doc i Span manualment, i actualitzaràs les entitats anomenades – igual que ho fa spaCy entre bastidors. Es crearà un objecte nlp compartit sense cap pipeline.\n",
    "\n",
    "- Importa les classes Doc i Span des de spacy.tokens.\n",
    "- Utilitza la classe Doc directament per crear un doc a partir de les paraules i espais.\n",
    "- Crea un Span per a \"David Bowie\" del doc i assigna-li l'etiqueta \"PERSON\".\n",
    "- Sobreescriu doc.ents amb una llista d'una entitat, l'span \"David Bowie\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.ca import Catalan\n",
    "\n",
    "nlp = Catalan()\n",
    "\n",
    "# Importa les classes Doc i Span\n",
    "from spacy.tokens import Doc, Span\n",
    "\n",
    "words = [\"M'\", \"encanta\", \"en\", \"David\", \"Bowie\"]\n",
    "spaces = [False, True, True, True, False]\n",
    "\n",
    "# Crea un doc a partir de les paraules i espais\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "print(doc.text)\n",
    "\n",
    "# Crea un span per a \"David Bowie\" del doc i assigna-li l'etiqueta \"PERSON\"\n",
    "span = Span(doc, 3, 5, label=\"PERSON\")\n",
    "print(span.text, span.label_)\n",
    "\n",
    "# Afegeix el span a les entitats del doc\n",
    "doc.ents = [span]\n",
    "\n",
    "# Imprimeix el text i les etiquetes de les entitats\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Estructures de dades i bones pràctiques\n",
    "\n",
    "El codi d'aquest exemple intenta analitzar un text i recollir tots els noms propis que van seguits d'un verb.\n",
    "\n",
    "### Part 1\n",
    "\n",
    "Per què el codi és dolent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Berlin is a nice city\")\n",
    "\n",
    "# Obté tots els tokens i etiquetes de part del discurs\n",
    "token_texts = [token.text for token in doc]\n",
    "pos_tags = [token.pos_ for token in doc]\n",
    "\n",
    "for index, pos in enumerate(pos_tags):\n",
    "    # Comprova si el token actual és un nom propi\n",
    "    if pos == \"PROPN\":\n",
    "        # Comprova si el següent token és un verb\n",
    "        if pos_tags[index + 1] == \"VERB\":\n",
    "            result = token_texts[index]\n",
    "            print(\"Trobat nom propi abans d'un verb:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(token_texts)\n",
    "print(pos_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(spacy.explain(\"PROPN\"))\n",
    "print(spacy.explain(\"VERB\"))\n",
    "print(spacy.explain(\"DET\"))\n",
    "print(spacy.explain(\"ADJ\"))\n",
    "print(spacy.explain(\"NOUN\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resposta: \n",
    "\n",
    "Només utilitza llistes de cadenes en lloc d'atributs natius de token. Això sovint és menys eficient, i no pot expressar relacions complexes.\n",
    "\n",
    "Converteix sempre els resultats a cadenes el més tard possible, i intenta utilitzar atributs natius de token per mantenir la consistència."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2\n",
    "Reescriu el codi per utilitzar els atributs natius de token en lloc de llistes de cadenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Berlin is a nice city\")\n",
    "\n",
    "for token in doc:\n",
    "    # Comprova si el token actual és un nom propi\n",
    "    if token.pos_ == \"PROPN\":\n",
    "        # Comprova si el següent token és un verb\n",
    "        if doc[token.i + 1].pos_ == \"VERB\":\n",
    "            print(\"Trobat nom propi abans d'un verb:\", token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Vectors de paraules i similituds semàntiques\n",
    "\n",
    "En aquesta lliçó, aprendràs a utilitzar spaCy per predir com de semblants són documents, spans o tokens entre si.\n",
    "\n",
    "També aprendràs sobre com utilitzar vectors de paraules i com treure'n profit a la teva aplicació de PLN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCy pot comparar dos objectes i predir com de semblants són – per exemple, documents, spans o tokens individuals.\n",
    "\n",
    "Els objectes Doc, Token i Span tenen un mètode dot similarity que pren un altre objecte i retorna un nombre de coma flotant entre 0 i 1, indicant com de semblants són.\n",
    "\n",
    "Una cosa molt important: Per utilitzar la similitud, necessites un model de spaCy més gran que inclogui vectors de paraules.\n",
    "\n",
    "<b>Per exemple, el model anglès mitjà o gran – però no el petit. Així que si vols utilitzar vectors, sempre utilitza un model que acabi en \"md\" o \"lg\". Pots trobar més detalls a la documentació de models.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- spaCy pot comparar dos objectes i predir la similitud\n",
    "- Doc.similarity(), Span.similarity() i Token.similarity()\n",
    "- Prenen un altre objecte i retornen un valor de similitud (entre 0 i 1)\n",
    "- Important: necessita un model que tingui vectors de paraules inclosos, per exemple en_core_web_md (mitjà) o en_core_web_lg (gran)\n",
    "- No en_core_web_sm (petit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Carrega un model més gran amb vectors\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# Compara dos documents\n",
    "doc1 = nlp(\"I like fast food\")\n",
    "doc2 = nlp(\"I like pizza\")\n",
    "print(doc1.similarity(doc2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compara dos tokens\n",
    "doc = nlp(\"I like pizza and pasta\")\n",
    "token1 = doc[2]\n",
    "token2 = doc[4]\n",
    "print(token1.similarity(token2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compara un document amb un token\n",
    "doc = nlp(\"I like pizza\")\n",
    "token = nlp(\"soap\")[0]\n",
    "\n",
    "print(doc.similarity(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compara un span amb un document\n",
    "span = nlp(\"I like pizza and pasta\")[2:5]\n",
    "doc = nlp(\"McDonalds sells burgers\")\n",
    "\n",
    "print(span.similarity(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Però com ho fa spaCy internament?\n",
    "\n",
    "La similitud es determina utilitzant vectors de paraules, representacions multidimensionals dels significats de les paraules.\n",
    "\n",
    "Potser has sentit parlar de Word2Vec, que és un algorisme que s'utilitza sovint per entrenar vectors de paraules a partir de text en brut.\n",
    "\n",
    "Els vectors es poden afegir als models estadístics de spaCy.\n",
    "\n",
    "Per defecte, la similitud retornada per spaCy és la similitud del cosinus entre dos vectors – però això es pot ajustar si és necessari.\n",
    "\n",
    "Els vectors per a objectes que consisteixen en diversos tokens, com el Doc i l'Span, per defecte són la mitjana dels seus vectors de token.\n",
    "\n",
    "Per això normalment obtens més valor de frases més curtes amb menys paraules irrellevants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com prediu spaCy la similitud?\n",
    "- La similitud es determina utilitzant vectors de paraules\n",
    "- Representacions multidimensionals del significat de les paraules\n",
    "- Generats utilitzant un algorisme com Word2Vec i molts textos\n",
    "- Es poden afegir als models estadístics de spaCy\n",
    "- Per defecte: similitud del cosinus, però es pot ajustar\n",
    "- Els vectors de Doc i Span per defecte són la mitjana dels vectors dels tokens\n",
    "- Les frases curtes són millors que les llargues amb moltes paraules irrellevants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"I have a banana\")\n",
    "# Accedeix al vector mitjançant l'atribut token.vector\n",
    "print(doc[3].vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### La similitud depèn del context de l'aplicació\n",
    "\n",
    "Útil per a moltes aplicacions: sistemes de recomanació, marcar duplicats, etc.\n",
    "\n",
    "No hi ha cap definició objectiva de \"similitud\"\n",
    "\n",
    "Depèn del context i del que l'aplicació necessiti fer.\n",
    "\n",
    "Aquí tens un exemple: els vectors de paraules per defecte de spaCy assignen una puntuació de similitud molt alta a \"I like cats\" i \"I hate cats\". Això té sentit, perquè tots dos textos expressen sentiment sobre gats. Però en un context d'aplicació diferent, potser voldries considerar les frases com a molt diferents, perquè parlen de sentiments oposats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = nlp(\"I like cats\")\n",
    "doc2 = nlp(\"I hate cats\")\n",
    "\n",
    "print(doc1.similarity(doc2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Útil per a moltes aplicacions: sistemes de recomanació, marcar duplicats, etc.\n",
    "- No hi ha cap definició objectiva de \"similitud\"\n",
    "- Depèn del context i del que l'aplicació necessiti fer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Inspeccionant vectors de paraules\n",
    "\n",
    "En aquest exercici, utilitzaràs un model anglès més gran, que inclou al voltant de 20.000 vectors de paraules. Com que els vectors triguen una mica més a carregar, estem utilitzant una versió lleugerament comprimida del que pots descarregar amb spaCy. El model ja està preinstal·lat.\n",
    "\n",
    "- Carrega el model mitjà 'en_core_web_md' amb vectors de paraules.\n",
    "- Imprimeix el vector per a \"bananas\" utilitzant l'atribut token.vector.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Carrega el model mitjà en_core_web_md\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# Processa un text\n",
    "doc = nlp(\"Two bananas in pyjamas\")\n",
    "\n",
    "# Obté el vector per al token \"bananas\"\n",
    "bananas_vector = doc[1].vector\n",
    "print(bananas_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Comparant similituds\n",
    "\n",
    "En aquest exercici, utilitzaràs el mètode similarity de spaCy per comparar objectes Doc, Token i Span i obtenir puntuacions de similitud.\n",
    "\n",
    "### Part 1\n",
    "\n",
    "- Utilitza el mètode doc.similarity per comparar doc1 amb doc2 i imprimeix el resultat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "doc1 = nlp(\"It's a warm summer day\")\n",
    "doc2 = nlp(\"It's sunny outside\")\n",
    "\n",
    "# Obté la similitud de doc1 i doc2\n",
    "similarity = doc1.similarity(doc2)\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2\n",
    "\n",
    "- Utilitza el mètode token.similarity per comparar token1 amb token2 i imprimeix el resultat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "doc = nlp(\"TV and books\")\n",
    "token1, token2 = doc[0], doc[2]\n",
    "\n",
    "# Obté la similitud del token \"TV\" i \"books\"\n",
    "similarity = token1.similarity(token2)\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3\n",
    "\n",
    "Crea spans per a \"great restaurant\"/\"really nice bar\".\n",
    "\n",
    "Utilitza span.similarity per comparar-los i imprimeix el resultat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "doc = nlp(\"This was a great restaurant. Afterwards, we went to a really nice bar.\")\n",
    "\n",
    "# Crea spans per a \"great restaurant\" i \"really nice bar\"\n",
    "span1 = doc[3:5]\n",
    "span2 = doc[12:15]\n",
    "\n",
    "# Obté la similitud dels spans\n",
    "similarity = span1.similarity(span2)\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"M'agrada una tassa de cafè calent però odio un pa fred amb mantega.\")\n",
    "\n",
    "span1 = doc[0:7]\n",
    "span2 = doc[8:15]\n",
    "\n",
    "# Obté la similitud dels spans\n",
    "similarity = span1.similarity(span2)\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Combinant models i regles\n",
    "\n",
    "Combinar models estadístics amb sistemes basats en regles és un dels trucs més potents que hauries de tenir a la teva caixa d'eines de PLN.\n",
    "\n",
    "En aquesta lliçó, farem un cop d'ull a com fer-ho amb spaCy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediccions estadístiques vs. regles \n",
    "\n",
    "Els models estadístics són útils si la teva aplicació necessita ser capaç de generalitzar basant-se en uns pocs exemples.\n",
    "\n",
    "Per exemple, detectar noms de productes o persones normalment es beneficia d'un model estadístic. En lloc de proporcionar una llista de tots els noms de persones de la història, la teva aplicació podrà predir si un span de tokens és un nom de persona. De manera similar, pots predir etiquetes de dependència per trobar relacions subjecte/objecte.\n",
    "\n",
    "Per fer això, utilitzaries el reconeixedor d'entitats, l'analitzador de dependències o l'etiquetador de parts del discurs de spaCy.\n",
    "\n",
    "Els enfocaments basats en regles, d'altra banda, són útils si hi ha un nombre més o menys finit d'instàncies que vols trobar. Per exemple, tots els països o ciutats del món, noms de medicaments o fins i tot races de gossos.\n",
    "\n",
    "A spaCy, pots aconseguir això amb regles de tokenització personalitzades, així com el matcher i phrase matcher."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "img src=\"img/vs.png\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RESUM: Coincidència basada en regles\n",
    "\n",
    "Al capítol anterior, vas aprendre a utilitzar el matcher basat en regles de spaCy per trobar patrons complexos als teus textos. Aquí tens un resum ràpid.\n",
    "\n",
    "El matcher s'inicialitza amb el vocabulari compartit – normalment nlp.vocab.\n",
    "\n",
    "Els patrons són llistes de diccionaris, i cada diccionari descriu un token i els seus atributs. Els patrons es poden afegir al matcher utilitzant el mètode matcher.add.\n",
    "\n",
    "Els operadors et permeten especificar quantes vegades coincidir un token. Per exemple, \"+\" coincidirà una o més vegades.\n",
    "\n",
    "Cridar el matcher sobre un objecte doc retornarà una llista de les coincidències. Cada coincidència és una tupla que consisteix en un ID, i l'índex de token d'inici i final al document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicialitza amb el vocabulari compartit\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Els patrons són llistes de diccionaris que descriuen els tokens\n",
    "pattern = [{\"LEMMA\": \"love\", \"POS\": \"VERB\"}, {\"LOWER\": \"cats\"}]\n",
    "matcher.add(\"LOVE_CATS\", None, pattern)\n",
    "\n",
    "# Els operadors poden especificar quantes vegades s'ha de coincidir un token\n",
    "pattern = [{\"TEXT\": \"very\", \"OP\": \"+\"}, {\"TEXT\": \"happy\"}]\n",
    "\n",
    "# Cridar matcher sobre doc retorna una llista de tuples (match_id, start, end)\n",
    "doc = nlp(\"I love cats and I'm very very happy\")\n",
    "matches = matcher(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RETORNA (list): Una llista de tuples `(key, start, end)`,\n",
    "    descrivint les coincidències. Una tupla de coincidència descriu un span\n",
    "    `doc[start:end]`. El `label_id` i `key` són tots dos enters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Afegint prediccions estadístiques \n",
    "\n",
    "Aquí tens un exemple d'una regla de matcher per a \"golden retriever\".\n",
    "\n",
    "Si iterem sobre les coincidències retornades pel matcher, podem obtenir l'ID de coincidència i l'índex d'inici i final del span coincident. Després podem descobrir més sobre ell. Els objectes Span ens donen accés al document original i a tots els altres atributs de token i característiques lingüístiques predites pel model.\n",
    "\n",
    "Per exemple, podem obtenir el token arrel del span. Si el span consisteix en més d'un token, aquest serà el token que decideix la categoria de la frase. Per exemple, l'arrel de \"Golden Retriever\" és \"Retriever\". També podem trobar el token cap de l'arrel. Aquest és el \"pare\" sintàctic que governa la frase – en aquest cas, el verb \"have\".\n",
    "\n",
    "Finalment, podem mirar el token anterior i els seus atributs. En aquest cas, és un determinant, l'article \"a\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add(\"DOG\", None, [{\"LOWER\": \"golden\"}, {\"LOWER\": \"retriever\"}])\n",
    "doc = nlp(\"I have a Golden Retriever\")\n",
    "\n",
    "for match_id, start, end in matcher(doc):\n",
    "    span = doc[start:end]\n",
    "    print(\"Span coincident:\", span.text)\n",
    "    # Obté el token arrel del span i el token cap de l'arrel\n",
    "    print(\"Token arrel:\", span.root.text)\n",
    "    print(\"Token cap de l'arrel:\", span.root.head.text)\n",
    "    # Obté el token anterior i la seva etiqueta POS\n",
    "    print(\"Token anterior:\", doc[start - 1].text, doc[start - 1].pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coincidència eficient de frases (1) \n",
    "\n",
    "El phrase matcher és una altra eina útil per trobar seqüències de paraules a les teves dades.\n",
    "\n",
    "Fa una cerca de paraules clau al document, però en lloc de trobar només cadenes, et dóna accés directe als tokens en context.\n",
    "\n",
    "Pren objectes Doc com a patrons.\n",
    "\n",
    "També és molt ràpid.\n",
    "\n",
    "Això el fa molt útil per coincidir grans diccionaris i llistes de paraules en grans volums de text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- PhraseMatcher com expressions regulars o cerca de paraules clau – però amb accés als tokens!\n",
    "- Pren objectes Doc com a patrons\n",
    "- Més eficient i ràpid que el Matcher\n",
    "- Ideal per coincidir grans llistes de paraules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí tens un exemple.\n",
    "\n",
    "El phrase matcher es pot importar des de spacy.matcher i segueix la mateixa API que el matcher normal.\n",
    "\n",
    "En lloc d'una llista de diccionaris, passem un objecte Doc com a patró.\n",
    "\n",
    "Després podem iterar sobre les coincidències al text, que ens dóna l'ID de coincidència, i l'inici i final de la coincidència. Això ens permet crear un objecte Span per als tokens coincidents \"Golden Retriever\" per analitzar-lo en context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "\n",
    "pattern = nlp(\"Golden Retriever\")\n",
    "matcher.add(\"DOG\", None, pattern)\n",
    "doc = nlp(\"I have a Golden Retriever\")\n",
    "\n",
    "# Itera sobre les coincidències\n",
    "for match_id, start, end in matcher(doc):\n",
    "    # Obté el span coincident\n",
    "    span = doc[start:end]\n",
    "    print(\"Span coincident:\", span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Depurant patrons (1)\n",
    "\n",
    "Per què aquest patró no coincideix amb els tokens \"Silicon Valley\" al doc?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = [{\"LOWER\": \"silicon\"}, {\"TEXT\": \" \"}, {\"LOWER\": \"valley\"}]\n",
    "\n",
    "doc = nlp(\"Can Silicon Valley workers.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Els tokens no inclouen espais. L'atribut de text d'un token retorna el text del token sense espais. No has de buscar espais!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Depurant patrons (2)\n",
    "\n",
    "Tots dos patrons d'aquest exercici contenen errors i no coincidiran com s'esperava. Pots arreglar-los? Si et quedes encallat, intenta imprimir els tokens al doc per veure com es divideix el text i ajusta el patró perquè cada diccionari representi un token.\n",
    "\n",
    "- Edita pattern1 perquè coincideixi correctament amb tots els casos de \"Amazon\" més un títol en majúscula i nom propi.\n",
    "- Edita pattern2 perquè coincideixi correctament amb totes les formes de \"ad-free\", més un substantiu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\n",
    "    \"Twitch Prime, the perks program for Amazon Prime members offering free \"\n",
    "    \"loot, games and other benefits, is ditching one of its best features: \"\n",
    "    \"ad-free viewing. According to an email sent out to Amazon Prime members \"\n",
    "    \"today, ad-free viewing will no longer be included as a part of Twitch \"\n",
    "    \"Prime for new members, beginning on September 14. However, members with \"\n",
    "    \"existing annual subscriptions will be able to continue to enjoy ad-free \"\n",
    "    \"viewing until their subscription comes up for renewal. Those with \"\n",
    "    \"monthly subscriptions will have access to ad-free viewing until October 15.\"\n",
    ")\n",
    "\n",
    "# Crea els patrons de coincidència\n",
    "pattern1 = [{\"LOWER\": \"Amazon\"}, {\"IS_TITLE\": True, \"POS\": \"PROPN\"}]\n",
    "pattern2 = [{\"LOWER\": \"ad-free\"}, {\"POS\": \"NOUN\"}]\n",
    "\n",
    "# Inicialitza el Matcher i afegeix els patrons\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add(\"PATTERN1\", None, pattern1)\n",
    "matcher.add(\"PATTERN2\", None, pattern2)\n",
    "\n",
    "# Itera sobre les coincidències\n",
    "for match_id, start, end in matcher(doc):\n",
    "    # Imprimeix el nom de cadena del patró i el text del span coincident\n",
    "    print(doc.vocab.strings[match_id], doc[start:end].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\n",
    "    \"Twitch Prime, the perks program for Amazon Prime members offering free \"\n",
    "    \"loot, games and other benefits, is ditching one of its best features: \"\n",
    "    \"ad-free viewing. According to an email sent out to Amazon Prime members \"\n",
    "    \"today, ad-free viewing will no longer be included as a part of Twitch \"\n",
    "    \"Prime for new members, beginning on September 14. However, members with \"\n",
    "    \"existing annual subscriptions will be able to continue to enjoy ad-free \"\n",
    "    \"viewing until their subscription comes up for renewal. Those with \"\n",
    "    \"monthly subscriptions will have access to ad-free viewing until October 15.\"\n",
    ")\n",
    "\n",
    "# Crea els patrons de coincidència\n",
    "pattern1 = [{\"LOWER\": \"amazon\"}, {\"IS_TITLE\": True, \"POS\": \"PROPN\"}]\n",
    "pattern2 = [{\"LOWER\": \"ad\"}, {\"TEXT\": \"-\"}, {\"LOWER\": \"free\"}, {\"POS\": \"NOUN\"}]\n",
    "\n",
    "# Inicialitza el Matcher i afegeix els patrons\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add(\"PATTERN1\", None, pattern1)\n",
    "matcher.add(\"PATTERN2\", None, pattern2)\n",
    "\n",
    "# Itera sobre les coincidències\n",
    "for match_id, start, end in matcher(doc):\n",
    "    # Imprimeix el nom de cadena del patró i el text del span coincident\n",
    "    print(doc.vocab.strings[match_id], doc[start:end].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Coincidència eficient de frases\n",
    "\n",
    "De vegades és més eficient coincidir cadenes exactes en lloc d'escriure patrons que descriguin els tokens individuals. Això és especialment cert per a categories finites de coses – com tots els països del món. Ja tenim una llista de països, així que utilitzem-la com a base del nostre script d'extracció d'informació. Una llista de noms de cadena està disponible com a variable COUNTRIES.\n",
    "\n",
    "- Importa el PhraseMatcher i inicialitza'l amb el vocabulari compartit com a variable matcher.\n",
    "- Afegeix els patrons de frase i crida el matcher sobre el doc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from spacy.lang.en import English\n",
    "\n",
    "with open(\"exercises/countries.json\") as f:\n",
    "    COUNTRIES = json.loads(f.read())\n",
    "\n",
    "nlp = English()\n",
    "doc = nlp(\"Czech Republic may help Slovakia protect its airspace\")\n",
    "\n",
    "# Import the PhraseMatcher and initialize it\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "\n",
    "# Create pattern Doc objects and add them to the matcher\n",
    "# This is the faster version of: [nlp(country) for country in COUNTRIES]\n",
    "patterns = list(nlp.pipe(COUNTRIES))\n",
    "matcher.add(\"COUNTRY\", None, *patterns)\n",
    "\n",
    "# Call the matcher on the test document and print the result\n",
    "matches = matcher(doc)\n",
    "print([doc[start:end] for match_id, start, end in matches])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraient països i relacions ----\n",
    "\n",
    "A l'exercici anterior, vas escriure un script utilitzant el PhraseMatcher de spaCy per trobar noms de països al text. Utilitzem aquest matcher de països en un text més llarg, analitzem la sintaxi i actualitzem les entitats del document amb els països coincidents.\n",
    "\n",
    "- Itera sobre les coincidències i crea un Span amb l'etiqueta \"GPE\" (entitat geopolítica).\n",
    "- Sobreescriu les entitats a doc.ents i afegeix el span coincident.\n",
    "- Obté el token cap de l'arrel del span coincident.\n",
    "- Imprimeix el text del token cap i el span."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from spacy.lang.en import English\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.tokens import Span\n",
    "\n",
    "with open(\"exercises/countries.json\") as f:\n",
    "    COUNTRIES = json.loads(f.read())\n",
    "with open(\"exercises/country_text.txt\") as f:\n",
    "    TEXT = f.read()\n",
    "\n",
    "nlp = English()\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "patterns = list(nlp.pipe(COUNTRIES))\n",
    "matcher.add(\"COUNTRY\", None, *patterns)\n",
    "\n",
    "# Crea un doc i troba coincidències\n",
    "doc = nlp(TEXT)\n",
    "\n",
    "# Itera sobre les coincidències\n",
    "for match_id, start, end in matcher(doc):\n",
    "    # Crea un Span amb l'etiqueta \"GPE\"\n",
    "    span = Span(doc, start, end, label=\"GPE\")\n",
    "\n",
    "    # Sobreescriu doc.ents i afegeix el span\n",
    "    doc.ents = list(doc.ents) + [span]\n",
    "\n",
    "    # Obté el token cap de l'arrel del span\n",
    "    span_root_head = span.root.head\n",
    "    # Imprimeix el text del token cap de l'arrel del span i el text del span\n",
    "    print(span_root_head.text, \"-->\", span.text)\n",
    "\n",
    "# Imprimeix les entitats del document\n",
    "print([(ent.text, ent.label_) for ent in doc.ents if ent.label_ == \"GPE\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contens",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "332.807px"
   },
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
